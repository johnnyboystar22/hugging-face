<!--Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->
# Pr√©traitement

Avant de pouvoir entra√Æner un mod√®le sur un ensemble de donn√©es, celui-ci doit √™tre pr√©trait√© pour correspondre au format d'entr√©e attendu par le mod√®le. Que vos donn√©es soient du texte, des images ou de l'audio, elles doivent √™tre converties et assembl√©es en s√©ries de tenseurs. ü§ó Transformers fournit un ensemble de classes de pr√©traitement pour vous aider √† pr√©parer vos donn√©es pour le mod√®le. Dans ce tutoriel, vous apprendrez √† :

* Pour le texte, utiliser un [Tokenizer](./main_classes/tokenizer) pour convertir le texte en une s√©quence de tokens, cr√©er une repr√©sentation num√©rique des tokens, et les assembler en tenseurs.
* Pour la parole et l'audio, utiliser un ["Feature Extractor"](./main_classes/feature_extractor) pour extraire des caract√©ristiques s√©quentielles des ondes audio et les convertir en tenseurs.
* Pour les entr√©es d'images, utiliser un ["Image Processor"](./main_classes/image_processor) pour convertir les images en tenseurs.
* Pour les entr√©es multimodales, utiliser un ["Processor"](./main_classes/processors) pour combiner un tokenizer et un extracteur de caract√©ristiques ou un processeur d'images.

<Tip>

`AutoProcessor` fonctionne **toujours** et choisit automatiquement la classe correcte pour le mod√®le que vous utilisez, que vous utilisiez un tokenizer, un processeur d'images, un extracteur de caract√©ristiques ou un processeur.

</Tip>

Avant de commencer, installez ü§ó Datasets afin de pouvoir charger quelques jeux de donn√©es pour exp√©rimenter :

```bash
pip install datasets
```

## Traitement du Langage Naturel

<Youtube id="Yffk5aydLzg"/>

L'outil principal pour le pr√©traitement des donn√©es textuelles est un [tokenizer](main_classes/tokenizer). Un tokenizer divise le texte en *tokens* selon un ensemble de r√®gles. Les tokens sont convertis en nombres puis en tenseurs, qui deviennent les entr√©es du mod√®le. Toute entr√©e suppl√©mentaire requise par le mod√®le est ajout√©e par le tokenizer.

<Tip>

Si vous pr√©voyez d'utiliser un mod√®le pr√©-entra√Æn√©, il est important d'utiliser le tokenizer pr√©-entra√Æn√© associ√©. Cela garantit que le texte est divis√© de la m√™me mani√®re que le corpus de pr√©-entra√Ænement, et utilise la m√™me correspondance tokens-index (g√©n√©ralement appel√©e *vocab*) que lors du pr√©-entra√Ænement.

</Tip>

Commencez par charger un tokenizer pr√©-entra√Æn√© avec la m√©thode [`AutoTokenizer.from_pretrained`]. Cela t√©l√©charge le *vocab* avec lequel un mod√®le a √©t√© pr√©-entra√Æn√© :

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
```

Puis passees votre texte au "*tokenizer*"

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```


Le tokenizer renvoie un dictionnaire avec trois √©l√©ments importants :

* [input_ids](glossary#input-ids) sont les indices correspondant √† chaque token dans la phrase.
* [attention_mask](glossary#attention-mask) indique si un token doit √™tre pris en compte ou non par le m√©canisme d'attention.
* [token_type_ids](glossary#token-type-ids) identifie √† quelle s√©quence appartient un token lorsqu'il y a plus d'une s√©quence.

R√©cup√©rez votre entr√©e en d√©codant les `input_ids` :

```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

Comme vous pouvez le voir, le tokenizer a ajout√© deux tokens sp√©ciaux - `CLS` et `SEP` (classificateur et s√©parateur) - √† la phrase. Tous les mod√®les n'ont pas besoin de tokens sp√©ciaux, mais s'ils en ont besoin, le tokenizer les ajoute automatiquement pour vous.

S'il y a plusieurs phrases que vous voulez pr√©traiter, passez-les sous forme de liste au tokenizer :

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1]]}
```

### Padding

Les phrases n'ont pas toujours la m√™me longueur, ce qui peut poser probl√®me car les entr√©es du mod√®le, repr√©sent√©es sous forme de tenseurs doivent avoir une forme uniforme. Le padding est une strat√©gie pour s'assurer que les tenseurs sont rectangulaires en ajoutant un *token de padding* sp√©cial aux phrases plus courtes.

D√©finissez le param√®tre `padding` √† `True` pour ajouter du padding aux s√©quences plus courtes du lot ("batch") afin qu'elles correspondent √† la s√©quence la plus longue :

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

Les premi√®re et troisi√®me phrases sont maintenant compl√©t√©es avec des `0` car elles sont plus courtes.

### Troncature

√Ä l'autre extr√©mit√© du spectre, parfois une s√©quence peut √™tre trop longue pour √™tre trait√©e par un mod√®le. Dans ce cas, vous devrez tronquer la s√©quence √† une longueur plus courte.

D√©finissez le param√®tre `truncation` √† `True` pour tronquer une s√©quence √† la longueur maximale accept√©e par le mod√®le :

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

<Tip>

Consultez le guide conceptuel ["Padding and truncation"](./pad_truncation) pour en savoir plus sur les diff√©rents arguments de padding et de troncature.

</Tip>

### Construction des tenseurs

Enfin, vous voulez que le tokenizer renvoie les tenseurs r√©els qui sont fournis au mod√®le.

D√©finissez le param√®tre `return_tensors` √† `pt` pour PyTorch, ou `tf` pour TensorFlow :

<frameworkcontent>
<pt>

```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
>>> print(encoded_input)
{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}
```
</pt>
<tf>
```py
>>> batch_sentences = [
...     "But what about second breakfast?",
...     "Don't think he knows about second breakfast, Pip.",
...     "What about elevensies?",
... ]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],
      dtype=int32)>,
 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,
 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}
```
</tf>
</frameworkcontent>

<Tip>

Les diff√©rents pipelines prennent en charge les arguments du tokenizer dans leur `__call__()` de mani√®re diff√©rente. Les pipelines `text-2-text-generation` ne prennent en charge (c'est-√†-dire ne transmettent) que `truncation`. Les pipelines `text-generation` prennent en charge `max_length`, `truncation`, `padding` et `add_special_tokens`.

Dans les pipelines `fill-mask`, les arguments du tokenizer peuvent √™tre pass√©s dans l'argument `tokenizer_kwargs` (dictionnaire).

</Tip>

## Audio

Pour les t√¢ches audio, vous aurez besoin d'un ["feature extractor"](main_classes/feature_extractor) pour pr√©parer votre jeu de donn√©es pour le mod√®le. L'extracteur de caract√©ristiques est con√ßu pour extraire des caract√©ristiques des donn√©es audio brutes et les convertir en tenseurs.

Chargez le jeu de donn√©es [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) (consultez le tutoriel ü§ó [Datasets](https://huggingface.co/docs/datasets/load_hub) pour plus de d√©tails sur la fa√ßon de charger un jeu de donn√©es) pour voir comment vous pouvez utiliser un extracteur de caract√©ristiques avec des jeux de donn√©es audio :

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
```

Acc√©dez au premier √©l√©ment de la colonne `audio` pour examiner l'entr√©e. L'appel de la colonne `audio` charge et r√©√©chantillonne automatiquement le fichier audio :

```py
>>> dataset[0]["audio"]
{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,
         0.        ,  0.        ], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 8000}
```

Cela retourne trois √©l√©ments :

* `array` est le signal vocal charg√© - et potentiellement r√©√©chantillonn√© - sous forme de tableau 1D.
* `path` pointe vers l'emplacement du fichier audio.
* `sampling_rate` fait r√©f√©rence au nombre de points de donn√©es dans le signal vocal mesur√©s par seconde.

Pour ce tutoriel, vous utiliserez le mod√®le [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base). En examinant la fiche du mod√®le, vous apprendrez que Wav2Vec2 est pr√©-entra√Æn√© sur de l'audio vocal √©chantillonn√© √† 16kHz. Il est important que le taux d'√©chantillonnage de vos donn√©es audio corresponde au taux d'√©chantillonnage du jeu de donn√©es utilis√© pour pr√©-entra√Æner le mod√®le. Si le taux d'√©chantillonnage de vos donn√©es n'est pas le m√™me, vous devez r√©√©chantillonner vos donn√©es.

1. Utilisez la m√©thode [`~datasets.Dataset.cast_column`] de ü§ó Datasets pour sur√©chantillonner le taux d'√©chantillonnage √† 16kHz :

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. Appelez √† nouveau la colonne `audio` pour r√©√©chantillonner le fichier audio :

```py
>>> dataset[0]["audio"]
{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,
         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',
 'sampling_rate': 16000}
```

Ensuite, chargez un extracteur de caract√©ristiques pour normaliser et ajouter du padding √† l'entr√©e. Lors de l'ajout de padding aux donn√©es textuelles, un `0` est ajout√© pour les s√©quences plus courtes. La m√™me id√©e s'applique aux donn√©es audio. L'extracteur de caract√©ristiques ajoute un `0` - interpr√©t√© comme du silence - √† `array`.

Chargez l'extracteur de caract√©ristiques avec [`AutoFeatureExtractor.from_pretrained`] :

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

Passez le `array` audio √† l'extracteur de caract√©ristiques. Nous recommandons √©galement d'ajouter l'argument `sampling_rate` dans l'extracteur de caract√©ristiques afin de mieux d√©boguer les erreurs silencieuses qui pourraient survenir.

```py
>>> audio_input = [dataset[0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,
        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}
```

Tout comme le tokenizer, vous pouvez appliquer du padding ou de la troncature pour g√©rer les s√©quences variables dans un lot. Jetez un ≈ìil √† la longueur de s√©quence de ces deux √©chantillons audio :

```py
>>> dataset[0]["audio"]["array"].shape
(173398,)

>>> dataset[1]["audio"]["array"].shape
(106496,)
```

Cr√©ez une fonction pour pr√©traiter le jeu de donn√©es afin que les √©chantillons audio aient la m√™me longueur. Sp√©cifiez une longueur maximale d'√©chantillon, et l'extracteur de caract√©ristiques ajoutera du padding ou tronquera les s√©quences pour correspondre √† cette longueur :

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x["array"] for x in examples["audio"]]
...     inputs = feature_extractor(
...         audio_arrays,
...         sampling_rate=16000,
...         padding=True,
...         max_length=100000,
...         truncation=True,
...     )
...     return inputs
```

Appliquez la `preprocess_function` aux premiers exemples du jeu de donn√©es :

```py
>>> processed_dataset = preprocess_function(dataset[:5])
```

Les longueurs des √©chantillons sont maintenant les m√™mes et correspondent √† la longueur maximale sp√©cifi√©e. Vous pouvez maintenant passer votre jeu de donn√©es trait√© au mod√®le !

```py
>>> processed_dataset["input_values"][0].shape
(100000,)

>>> processed_dataset["input_values"][1].shape
(100000,)
```

## Vision par ordinateur

Pour les t√¢ches de vision par ordinateur, vous aurez besoin d'un [processeur d'images](main_classes/image_processor) pour pr√©parer votre jeu de donn√©es pour le mod√®le. Le pr√©traitement des images consiste en plusieurs √©tapes qui convertissent les images en l'entr√©e attendue par le mod√®le. Ces √©tapes incluent, sans s'y limiter, le redimensionnement, la normalisation, la correction des canaux de couleur et la conversion des images en tenseurs.

<Tip>

Le pr√©traitement des images suit souvent une forme d'augmentation d'image. Le pr√©traitement des images et l'augmentation des images transforment tous deux les donn√©es d'image, mais ils servent des objectifs diff√©rents :

* L'augmentation d'image modifie les images d'une mani√®re qui peut aider √† pr√©venir le surapprentissage et augmenter la robustesse du mod√®le. Vous pouvez √™tre cr√©atif dans la fa√ßon dont vous augmentez vos donn√©es - ajustez la luminosit√© et les couleurs, recadrez, faites pivoter, redimensionnez, zoomez, etc. Cependant, veillez √† ne pas changer le sens des images avec vos augmentations.
* Le pr√©traitement des images garantit que les images correspondent au format d'entr√©e attendu par le mod√®le. Lors du fine-tuning d'un mod√®le de vision par ordinateur, les images doivent √™tre pr√©trait√©es exactement comme lors de l'entra√Ænement initial du mod√®le.

Vous pouvez utiliser n'importe quelle biblioth√®que pour l'augmentation d'image. Pour le pr√©traitement des images, utilisez l'`ImageProcessor` associ√© au mod√®le.

</Tip>

Chargez le jeu de donn√©es [food101](https://huggingface.co/datasets/food101) (voir le tutoriel ü§ó [Datasets](https://huggingface.co/docs/datasets/load_hub) pour plus de d√©tails sur la fa√ßon de charger un jeu de donn√©es) pour voir comment vous pouvez utiliser un processeur d'images avec des jeux de donn√©es de vision par ordinateur :

<Tip>

Utilisez le param√®tre `split` de ü§ó Datasets pour ne charger qu'un petit √©chantillon de la partition d'entra√Ænement car le jeu de donn√©es est assez volumineux !

</Tip>

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split="train[:100]")
```

Ensuite, examinez l'image avec la fonctionnalit√© [`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image) de ü§ó Datasets :

```py
>>> dataset[0]["image"]
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png"/>
</div>

Chargez le processeur d'images avec [`AutoImageProcessor.from_pretrained`] :

```py
>>> from transformers import AutoImageProcessor

>>> image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
```

Tout d'abord, ajoutons de l'augmentation d'image. Vous pouvez utiliser la biblioth√®que de votre choix, mais dans ce tutoriel, nous utiliserons le module [`transforms`](https://pytorch.org/vision/stable/transforms.html) de torchvision. Si vous souhaitez utiliser une autre biblioth√®que d'augmentation de donn√©es, apprenez comment faire dans les notebooks [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb) ou [Kornia](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb).

1. Ici, nous utilisons [`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html) pour encha√Æner quelques transformations - [`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) et [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html). Notez que pour le redimensionnement, nous pouvons obtenir les exigences de taille d'image √† partir de l'`image_processor`. Pour certains mod√®les, une hauteur et une largeur exactes sont attendues, pour d'autres, seul le `shortest_edge` est d√©fini.

```py
>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

>>> size = (
...     image_processor.size["shortest_edge"]
...     if "shortest_edge" in image_processor.size
...     else (image_processor.size["height"], image_processor.size["width"])
... )

>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])
```

2. Le mod√®le accepte [`pixel_values`](model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) comme entr√©e. `ImageProcessor` peut se charger de normaliser les images et de g√©n√©rer les tenseurs appropri√©s. Cr√©ez une fonction qui combine l'augmentation d'image et le pr√©traitement d'image pour un lot d'images et g√©n√®re `pixel_values` :

```py
>>> def transforms(examples):
...     images = [_transforms(img.convert("RGB")) for img in examples["image"]]
...     examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
...     return examples
```

<Tip>

Dans l'exemple ci-dessus, nous avons d√©fini `do_resize=False` car nous avons d√©j√† redimensionn√© les images dans la transformation d'augmentation d'image, et utilis√© l'attribut `size` du `image_processor` appropri√©. Si vous ne redimensionnez pas les images pendant l'augmentation d'image, omettez ce param√®tre. Par d√©faut, `ImageProcessor` g√©rera le redimensionnement.

Si vous souhaitez normaliser les images dans le cadre de la transformation d'augmentation, utilisez les valeurs `image_processor.image_mean` et `image_processor.image_std`.
</Tip>

3. Ensuite, utilisez [`~datasets.Dataset.set_transform`] de ü§ó Datasets pour appliquer les transformations √† la vol√©e :
```py
>>> dataset.set_transform(transforms)
```

4. Maintenant, lorsque vous acc√©dez √† l'image, vous remarquerez que le processeur d'images a ajout√© `pixel_values`. Vous pouvez maintenant passer votre jeu de donn√©es trait√© au mod√®le !

```py
>>> dataset[0].keys()
```

Voici √† quoi ressemble l'image apr√®s l'application des transformations. L'image a √©t√© recadr√©e al√©atoirement et ses propri√©t√©s de couleur sont diff√©rentes.

```py
>>> import numpy as np
>>> import matplotlib.pyplot as plt

>>> img = dataset[0]["pixel_values"]
>>> plt.imshow(img.permute(1, 2, 0))
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png"/>
</div>

<Tip>

Pour des t√¢ches comme la d√©tection d'objets, la segmentation s√©mantique, la segmentation d'instances et la segmentation panoptique, `ImageProcessor` offre des m√©thodes de post-traitement. Ces m√©thodes convertissent les sorties brutes du mod√®le en pr√©dictions significatives telles que des bo√Ætes englobantes ou des cartes de segmentation.

</Tip>

### Padding

Dans certains cas, par exemple lors du fine-tuning de [DETR](./model_doc/detr), le mod√®le applique une augmentation d'√©chelle pendant l'entra√Ænement. Cela peut entra√Æner des images de tailles diff√©rentes dans un lot. Vous pouvez utiliser [`DetrImageProcessor.pad`] de [`DetrImageProcessor`] et d√©finir une `collate_fn` personnalis√©e pour regrouper les images ensemble.

```py
>>> def collate_fn(batch):
...     pixel_values = [item["pixel_values"] for item in batch]
...     encoding = image_processor.pad(pixel_values, return_tensors="pt")
...     labels = [item["labels"] for item in batch]
...     batch = {}
...     batch["pixel_values"] = encoding["pixel_values"]
...     batch["pixel_mask"] = encoding["pixel_mask"]
...     batch["labels"] = labels
...     return batch
```

## Multimodal

Pour les t√¢ches impliquant des entr√©es multimodales, vous aurez besoin d'un [processeur](main_classes/processors) pour pr√©parer votre jeu de donn√©es pour le mod√®le. Un processeur couple deux objets de traitement tels qu'un tokenizer et un extracteur de caract√©ristiques.

Chargez le jeu de donn√©es [LJ Speech](https://huggingface.co/datasets/lj_speech) (voir le tutoriel ü§ó [Datasets](https://huggingface.co/docs/datasets/load_hub) pour plus de d√©tails sur la fa√ßon de charger un jeu de donn√©es) pour voir comment vous pouvez utiliser un processeur pour la reconnaissance automatique de la parole (ASR) :

```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset("lj_speech", split="train")
```

Pour l'ASR, vous vous concentrez principalement sur `audio` et `text`, donc vous pouvez supprimer les autres colonnes :

```py
>>> lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])
```

Maintenant, examinez les colonnes `audio` et `text` :

```py
>>> lj_speech[0]["audio"]
{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,
         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',
 'sampling_rate': 22050}

>>> lj_speech[0]["text"]
'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'
```

N'oubliez pas que vous devez toujours [r√©√©chantillonner](preprocessing#audio) le taux d'√©chantillonnage de votre jeu de donn√©es audio pour correspondre au taux d'√©chantillonnage du jeu de donn√©es utilis√© pour pr√©-entra√Æner un mod√®le !

```py
>>> lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))
```

Chargez un processeur avec [`AutoProcessor.from_pretrained`] :

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. Cr√©ez une fonction pour traiter les donn√©es audio contenues dans `array` en `input_values`, et tokeniser `text` en `labels`. Ce sont les entr√©es du mod√®le :

```py
>>> def prepare_dataset(example):
...     audio = example["audio"]

...     example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

...     return example
```

2. Appliquez la fonction `prepare_dataset` √† un √©chantillon :

```py
>>> prepare_dataset(lj_speech[0])
```

Le processeur a maintenant ajout√© `input_values` et `labels`, et le taux d'√©chantillonnage a √©galement √©t√© correctement sous-√©chantillonn√© √† 16kHz. Vous pouvez maintenant passer votre jeu de donn√©es trait√© au mod√®le !
