from ...modeling_utils import PreTrainedModel
from .configuration_msclap import MSClapConfig, MSClapTextConfig, MSClapAudioConfig
import torch.nn as nn
import math
import torch
import warnings
from typing import Optional, Union, Tuple



def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)



class MSClapPreTrainedModel(PreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """

    config_class = MSClapConfig
    base_model_prefix = "msclap"
    supports_gradient_checkpointing = False

    def _init_weights(self, module):
        
        # if isinstance(module, ClapTextEmbeddings):
        #     module.position_embeddings.weight.data.normal_(mean=0.0, std=factor * 0.02)
        #     module.token_type_embeddings.weight.data.normal_(mean=0.0, std=factor * 0.02)
        # elif isinstance(module, ClapModel):
        #     nn.init.normal_(module.logit_scale_a, std=factor * 0.02)
        #     nn.init.normal_(module.logit_scale_t, std=factor * 0.02)
        # elif isinstace(module, nn.Embedding):
        #     module.weight.data.normal_(mean=0.0, std=factor * 0.02)

        if isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        elif isinstance(module, (nn.Conv2d, nn.Linear)):
            in_proj_std = (self.config.hidden_size**-0.5) * ((2 * self.config.num_hidden_layers) ** -0.5) * self.config.initializer_factor
            nn.init.normal_(module.weight, std=in_proj_std)
            if module.bias is not None:
                module.bias.data.zero_()


class MSClapTextModel(MSClapPreTrainedModel): 

    config_class = MSClapTextConfig

    pass

class MSClapAudioModel(MSClapPreTrainedModel): 

    pass 


class MSClapProjectionLayer(MSClapPreTrainedModel): 

    pass 

class Projection(nn.Module):
    def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:
        super().__init__()
        self.linear1 = nn.Linear(d_in, d_out, bias=False)
        self.linear2 = nn.Linear(d_out, d_out, bias=False)
        self.layer_norm = nn.LayerNorm(d_out)
        self.drop = nn.Dropout(p)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        embed1 = self.linear1(x)
        embed2 = self.drop(self.linear2(F.gelu(embed1)))
        embeds = self.layer_norm(embed1 + embed2)
        return embeds




class MSClapModel(MSClapPreTrainedModel):
    config_class = MSClapConfig

    def __init__(self, config: MSClapConfig):
        super().__init__(config)

        if not isinstance(config.text_config, MSClapTextConfig):
            raise ValueError(
                "config.text_config is expected to be of type MSClapTextConfig but is of type"
                f" {type(config.text_config)}."
            )

        if not isinstance(config.audio_config, MSClapAudioConfig):
            raise ValueError(
                "config.audio_config is expected to be of type MSClapAudioConfig but is of type"
                f" {type(config.audio_config)}."
            )


        text_config = config.text_config
        audio_config = config.audio_config

        self.logit_scale = nn.Parameter(torch.tensor(math.log(config.logit_scale_init_value)))

        self.projection_dim = config.projection_dim

        self.text_model = MSClapTextModel(text_config)
        self.text_projection = MSClapProjectionLayer(text_config)

        self.audio_model = MSClapAudioModel(audio_config)
        self.audio_projection = MSClapProjectionLayer(audio_config)

        # Initialize weights and apply final processing
        self.post_init()

    # def get_text_features(
    #     self,
    #     input_ids: Optional[torch.Tensor] = None,
    #     attention_mask: Optional[torch.Tensor] = None,
    #     position_ids: Optional[torch.Tensor] = None,
    #     output_attentions: Optional[bool] = None,
    #     output_hidden_states: Optional[bool] = None,
    #     return_dict: Optional[bool] = None,
    # ) -> torch.FloatTensor:
    #     r"""
    #     Returns:
    #         text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by
    #         applying the projection layer to the pooled output of [`MSClapTextModel`].

    #     Examples:

    #     ```python
    #     >>> from transformers import AutoTokenizer, MSClapModel

    #     >>> model = MSClapModel.from_pretrained("laion/MSClap-htsat-unfused")
    #     >>> tokenizer = AutoTokenizer.from_pretrained("laion/MSClap-htsat-unfused")

    #     >>> inputs = tokenizer(["the sound of a cat", "the sound of a dog"], padding=True, return_tensors="pt")
    #     >>> text_features = model.get_text_features(**inputs)
    #     ```"""
        # Use MSClap model's config for some fields (if specified) instead of those of audio & text components.
        # output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        # output_hidden_states = (
        #     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        # )
        # return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # text_outputs = self.text_model(
        #     input_ids=input_ids,
        #     attention_mask=attention_mask,
        #     position_ids=position_ids,
        #     output_attentions=output_attentions,
        #     output_hidden_states=output_hidden_states,
        #     return_dict=return_dict,
        # )

        # pooled_output = text_outputs[1] if return_dict is not None else text_outputs.pooler_output
        # text_features = self.text_projection(pooled_output)
        # text_features = F.normalize(text_features, dim=-1)
        # return text_features

    # def get_audio_features(
    #     self,
    #     input_features: Optional[torch.Tensor] = None,
    #     is_longer: Optional[torch.Tensor] = None,
    #     attention_mask: Optional[torch.Tensor] = None,
    #     output_attentions: Optional[bool] = None,
    #     output_hidden_states: Optional[bool] = None,
    #     return_dict: Optional[bool] = None,
    # ) -> torch.FloatTensor:
    #     r"""
    #     Returns:
    #         audio_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The audio embeddings obtained by
    #         applying the projection layer to the pooled output of [`MSClapAudioModel`].

    #     Examples:

    #     ```python
    #     >>> from transformers import AutoFeatureExtractor, MSClapModel
    #     >>> import torch

    #     >>> model = MSClapModel.from_pretrained("laion/MSClap-htsat-unfused")
    #     >>> feature_extractor = AutoFeatureExtractor.from_pretrained("laion/MSClap-htsat-unfused")
    #     >>> random_audio = torch.rand((16_000))
    #     >>> inputs = feature_extractor(random_audio, return_tensors="pt")
    #     >>> audio_features = model.get_audio_features(**inputs)
    #     ```"""
    #     output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
    #     output_hidden_states = (
    #         output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
    #     )
    #     return_dict = return_dict if return_dict is not None else self.config.use_return_dict

    #     audio_outputs = self.audio_model(
    #         input_features=input_features,
    #         is_longer=is_longer,
    #         return_dict=return_dict,
    #     )

    #     pooled_output = audio_outputs[1] if not return_dict else audio_outputs.pooler_output

    #     audio_features = self.audio_projection(pooled_output)
    #     audio_features = F.normalize(audio_features, dim=-1)

    #     return audio_features


    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        input_features: Optional[torch.FloatTensor] = None,
        is_longer: Optional[torch.BoolTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        return_loss: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
    ) -> Union[Tuple, MSClapOutput]:
        r"""
        Returns:

        Examples:

        ```python
        >>> from datasets import load_dataset
        >>> from transformers import AutoProcessor, MSClapModel

        >>> dataset = load_dataset("hf-internal-testing/ashraq-esc50-1-dog-example")
        >>> audio_sample = dataset["train"]["audio"][0]["array"]

        >>> model = MSClapModel.from_pretrained("laion/MSClap-htsat-unfused")
        >>> processor = AutoProcessor.from_pretrained("laion/MSClap-htsat-unfused")

        >>> input_text = ["Sound of a dog", "Sound of vaccum cleaner"]

        >>> inputs = processor(text=input_text, audios=audio_sample, return_tensors="pt", padding=True)

        >>> outputs = model(**inputs)
        >>> logits_per_audio = outputs.logits_per_audio  # this is the audio-text similarity score
        >>> probs = logits_per_audio.softmax(dim=-1)  # we can take the softmax to get the label probabilities
        ```"""
        # Use MSClap model's config for some fields (if specified) instead of those of audio & text components.
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        audio_outputs = self.audio_model(
            input_features=input_features,
            is_longer=is_longer,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        text_outputs = self.text_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        audio_embeds = audio_outputs[1] if not return_dict else audio_outputs.pooler_output
        audio_embeds = self.audio_projection(audio_embeds)

        text_embeds = text_outputs[1] if not return_dict else text_outputs.pooler_output
        text_embeds = self.text_projection(text_embeds)

        # normalized features
        audio_embeds = audio_embeds / audio_embeds.norm(p=2, dim=-1, keepdim=True)
        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)

        # cosine similarity as logits
        logit_scale_text = self.logit_scale_t.exp()
        logit_scale_audio = self.logit_scale_a.exp()
        logits_per_text = torch.matmul(text_embeds, audio_embeds.t()) * logit_scale_text
        logits_per_audio = torch.matmul(audio_embeds, text_embeds.t()) * logit_scale_audio

        loss = None
        if return_loss:
            caption_loss = contrastive_loss(logits_per_text)
            audio_loss = contrastive_loss(logits_per_audio.t())
            loss = (caption_loss + audio_loss) / 2.0

        if not return_dict:
            output = (logits_per_audio, logits_per_text, text_embeds, audio_embeds, text_outputs, audio_outputs)
            return ((loss,) + output) if loss is not None else output

        return MSClapOutput(
            loss=loss,
            logits_per_audio=logits_per_audio,
            logits_per_text=logits_per_text,
            text_embeds=text_embeds,
            audio_embeds=audio_embeds,
            text_model_output=text_outputs,
            audio_model_output=audio_outputs,
        )





